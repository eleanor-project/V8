# ELEANOR V8 - GPU Acceleration Configuration

gpu:
  # Enable GPU acceleration
  enabled: true
  
  # Device preference order (first available is used)
  # Options: cuda (NVIDIA), mps (Apple Silicon), cpu
  device_preference: ["cuda", "mps", "cpu"]
  
  # Multi-GPU settings
  multi_gpu:
    enabled: false
    device_ids: [0, 1, 2, 3]  # Which GPUs to use
    strategy: "data_parallel"  # data_parallel, model_parallel, or pipeline_parallel
  
  # Memory optimization
  memory:
    # Mixed precision training (FP16/BF16)
    mixed_precision: true
    precision: "fp16"  # fp16, bf16, or fp32
    
    # Model quantization for reduced memory
    quantization:
      enabled: false
      bits: 8  # 4, 8, or 16
      method: "dynamic"  # dynamic or static
    
    # Memory limits
    max_memory_per_gpu: "24GB"
    offload_to_cpu: false
    kv_cache_size: "4GB"
  
  # Batch processing
  batching:
    enabled: true
    default_batch_size: 8
    max_batch_size: 32
    dynamic_batching: true  # Adjust batch size based on memory
  
  # Async operations
  async:
    num_streams: 4  # Number of CUDA streams for parallel ops
    prefetch: true  # Prefetch next batch while processing current
  
  # Monitoring
  monitoring:
    log_memory_stats: true
    memory_check_interval: 60  # seconds
    enable_profiling: false  # Detailed performance profiling

# LLM Backend GPU Configuration
ollama:
  gpu_layers: -1  # -1 = all layers on GPU, 0 = CPU only, N = N layers on GPU
  num_gpu: 1  # Number of GPUs to use per model
  gpu_memory_utilization: 0.9  # Max fraction of GPU memory to use

# Embedding model GPU settings  
embeddings:
  device: "cuda"  # cuda, mps, or cpu
  batch_size: 32
  cache_on_gpu: true  # Keep embedding cache on GPU
  max_cache_size_gb: 2
