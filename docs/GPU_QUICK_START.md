# GPU Acceleration - Quick Start Guide

## Overview

ELEANOR V8 includes GPU acceleration for:
- üöÄ **5-15x faster inference**
- ‚ö° **10-50x faster embeddings**
- üí™ **Multi-GPU support**
- üçé **Apple Silicon (M1/M2/M3) support**

---

## Installation

### Basic GPU Support

```bash
# Install GPU dependencies
pip install -r requirements-gpu.txt

# This includes:
# - torch (with CUDA support on Linux/Windows)
# - torchvision
# - nvidia-ml-py3 (NVIDIA GPUs)
# - gputil
```

### Optional: Performance Optimizations

```bash
# Install advanced optimizations
pip install -r requirements-gpu-optimized.txt

# This includes:
# - flash-attn (FlashAttention)
# - bitsandbytes (quantization)
# - accelerate (multi-GPU)
```

---

## Quick Start

### 1. Check GPU Availability

```python
from engine.gpu import GPUManager

# Initialize GPU manager
gpu = GPUManager()

# Check if GPU is available
if gpu.is_gpu_available():
    print(f"‚úÖ GPU available: {gpu.device}")
    print(f"   Device count: {gpu.devices_available}")
    
    # Get memory stats
    stats = gpu.memory_stats()
    print(f"   Memory: {stats['allocated_mb']:.1f} MB / {stats['total_mb']:.1f} MB")
else:
    print("‚ùå No GPU available (using CPU)")
```

### 2. Use Async GPU Operations

```python
import asyncio
from engine.gpu import GPUManager, AsyncGPUExecutor

async def main():
    # Initialize
    gpu = GPUManager()
    executor = AsyncGPUExecutor(gpu.device, num_streams=4)
    
    # Execute GPU operation asynchronously
    def gpu_operation(x):
        # Your GPU computation here
        return x * 2
    
    result = await executor.execute_async(gpu_operation, 42)
    print(f"Result: {result}")
    
    # Batch execution
    operations = [
        (gpu_operation, (1,), {}),
        (gpu_operation, (2,), {}),
        (gpu_operation, (3,), {}),
    ]
    
    results = await executor.batch_execute(operations)
    print(f"Batch results: {results}")

# Run
asyncio.run(main())
```

### 3. Monitor GPU Health

```python
from engine.gpu import GPUManager

gpu = GPUManager()

# Health check
health = gpu.health_check()
print(f"GPU Health: {health}")

# Per-device stats (CUDA only)
if gpu.device.type == "cuda":
    for device_id in range(gpu.devices_available):
        stats = gpu.memory_stats(device_id)
        print(f"GPU {device_id}: {stats['utilization_pct']:.1f}% utilization")
```

---

## Configuration

### GPU Configuration File: `config/gpu.yaml`

```yaml
gpu:
  enabled: true
  device_preference: ["cuda", "mps", "cpu"]
  
  multi_gpu:
    enabled: false
    device_ids: [0, 1]
  
  memory:
    mixed_precision: true
    precision: "fp16"
  
  batching:
    enabled: true
    default_batch_size: 8
  
  async:
    num_streams: 4
```

---

## Platform-Specific Setup

### NVIDIA GPUs (Linux/Windows)

**Requirements:**
- NVIDIA GPU (GTX 1660 or better)
- CUDA 11.8+ or CUDA 12.x
- NVIDIA drivers 525.60.13+

**Install:**
```bash
# Install PyTorch with CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Verify
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### Apple Silicon (macOS)

**Requirements:**
- M1, M2, or M3 Mac
- macOS 12.3+

**Install:**
```bash
# Install PyTorch with MPS support
pip install torch torchvision

# Verify
python -c "import torch; print(f'MPS available: {torch.backends.mps.is_available()}')"
```

### CPU Fallback

If no GPU is available, ELEANOR automatically falls back to CPU:

```bash
# CPU-only PyTorch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
```

---

## Performance Tips

### 1. Enable Mixed Precision

```yaml
# config/gpu.yaml
gpu:
  memory:
    mixed_precision: true
    precision: "fp16"  # or "bf16" for A100/H100
```

**Benefit:** 2x speedup, 50% memory reduction

### 2. Use Multiple Streams

```python
# More streams = better parallelization
executor = AsyncGPUExecutor(device, num_streams=8)
```

**Benefit:** 20-30% throughput improvement

### 3. Batch Operations

```python
# Process multiple items together
operations = [(compute, (item,), {}) for item in batch]
results = await executor.batch_execute(operations)
```

**Benefit:** 3-5x throughput improvement

### 4. Monitor Memory

```python
# Reset peak stats to track per-request usage
gpu.reset_peak_stats()

# Your GPU operation
result = await process_with_gpu(...)

# Check peak memory
stats = gpu.memory_stats()
print(f"Peak memory: {stats['max_allocated_mb']:.1f} MB")
```

---

## Troubleshooting

### "CUDA out of memory"

**Solutions:**
1. Reduce batch size in `config/gpu.yaml`
2. Enable mixed precision (`fp16`)
3. Offload to CPU: `offload_to_cpu: true`
4. Use quantization: `quantization.enabled: true`

### "torch not available"

```bash
# Reinstall PyTorch
pip uninstall torch torchvision
pip install torch torchvision
```

### "No GPU detected"

**Check:**
```bash
# NVIDIA
nvidia-smi

# Verify drivers
python -c "import torch; print(torch.cuda.is_available())"
```

### "Slow performance on Mac"

- Ensure MPS is enabled (macOS 12.3+)
- Check Activity Monitor for GPU usage
- Some operations may be faster on CPU for small batches

---

## Next Steps

- Read [`docs/GPU_ARCHITECTURE.md`](GPU_ARCHITECTURE.md) for technical details
- See [`examples/gpu_example.py`](../examples/gpu_example.py) for full examples
- Check Issue #25 for roadmap and advanced features

---

**Status:** Phase 1 & 2 Complete ‚úÖ  
**Coming Next:** GPU-accelerated embeddings, batched critic execution
